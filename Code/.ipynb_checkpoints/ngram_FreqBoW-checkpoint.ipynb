{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function get_freqBoW_and_targetvect\n",
    "This function obtains the <b>frequency bag-of-words matrix</b> representation and the <b>target vector</b> <u>for both training and test sets</u> of a given dataset.\n",
    "\n",
    "Parameters:\n",
    "1. dataset: string{\"ag_news\", \"dbpedia\", \"sogou_news\", \"yahoo\", \"amazon_full\", \"amazon_polarity\"}\n",
    "\n",
    "2. vocab_length: int, default=5000\n",
    "   \n",
    "   Represents the number of features in the frequency bag-of-words matrix (or, similarly, the number of words in the vocabulary)\n",
    "   \n",
    "3. ngram_min_max: tuple (min_n, max_n), default=(1,1).\n",
    "   \n",
    "   The lower and upper boundary of the range of n-values for different n-grams to be extracted. Examples: \n",
    "    - A value of (1,1) represents a 1-gram or single word,\n",
    "    - A value of (2,2) represents a bigram,\n",
    "    - And a value of (1,2) takes both 1-grams and bigrams (up to the number of features given by vocab_length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freqBoW_and_targetvect(dataset, vocab_length=5000, ngram_min_max=(1,1)):\n",
    "    \n",
    "    # Dictionary of the train and test files for all the datasets\n",
    "    dict_datasets_files = {\n",
    "        \"ag_news\": [\"../Datasets/ag_news_csv/train.csv\", \"../Datasets/ag_news_csv/test.csv\"],\n",
    "        \"dbpedia\": [\"../Datasets/dbpedia_csv/train.csv\", \"../Datasets/dbpedia_csv/test.csv\"],\n",
    "        \"sogou_news\": [\"../Datasets/sogou_news_csv/train.csv\", \"../Datasets/sogou_news_csv/test.csv\"],\n",
    "        \"yahoo\": [\"../Datasets/yahoo_answers_csv/train.csv\", \"../Datasets/yahoo_answers_csv/test.csv\"],\n",
    "        \"amazon_full\": [\"../Datasets/amazon_review_full_csv/train.csv\", \"../Datasets/amazon_review_full_csv/test.csv\"],\n",
    "        \"amazon_polarity\": [\"../Datasets/amazon_review_polarity_csv/train.csv\", \"../Datasets/amazon_review_polarity_csv/test.csv\"],\n",
    "        #\"prb\": [\"./datos_de_prueba.csv\", \"./datos_de_prueba_copy.csv\"]\n",
    "    }\n",
    "    \n",
    "    # Get the corresponding train and test files according to the dataset chosen\n",
    "    file_train, file_test = dict_datasets_files[dataset]   \n",
    "    \n",
    "    \n",
    "    # TRAIN (original)\n",
    "\n",
    "    # I. Get frequency bag-of-words:\n",
    "    # I-1. Training data\n",
    "    data = pd.read_csv(file_train, header=None)\n",
    "    #print(\"TRAIN DATA\")\n",
    "    #print(data)\n",
    "    # Combining the columns of text (eliminating the entries with 'Nan' value)\n",
    "    if dataset == \"yahoo\":\n",
    "        lines = data.loc[:,1].fillna(\"\") + \" \" + data.loc[:,2].fillna(\"\") + \" \" + data.loc[:,3].fillna(\"\") # 3-columns text\n",
    "    else:\n",
    "        lines = data.loc[:,1].fillna(\"\") + \" \" + data.loc[:,2].fillna(\"\") # 2-columns text\n",
    "    #print(lines)\n",
    "    \n",
    "    # I-2. Initialize the CountVectorizer, it will retrieve the frequency bag-of-words for the top n most common words (corresponding to the value stored in the vocab_length parameter)\n",
    "    if dataset == \"sogou_news\":\n",
    "        # For Chinesse news, don't use a any stop words\n",
    "        vectorizer_train = CountVectorizer(stop_words=None, max_features = vocab_length, ngram_range = ngram_min_max)\n",
    "    else:\n",
    "        # For the other datasets, use the English most common stop words\n",
    "        vectorizer_train = CountVectorizer(stop_words='english', max_features = vocab_length, ngram_range = ngram_min_max)\n",
    "\n",
    "    # I-3. Apply this CountVectorizer to the training data\n",
    "    x_train = vectorizer_train.fit_transform(lines)\n",
    "    \n",
    "    # II. Get the target vector:\n",
    "    y_train = data.loc[:,0].as_matrix()\n",
    "    \n",
    "\n",
    "    # TEST\n",
    "\n",
    "    # I. Get frequency bag-of-words:\n",
    "    # I-1. Test data\n",
    "    data = pd.read_csv(file_test, header=None)\n",
    "    #print(\"TEST DATA\")\n",
    "    #print(data)\n",
    "    # Combining the columns of text (eliminating the entries with 'Nan' value)\n",
    "    if dataset == \"yahoo\":\n",
    "        lines = data.loc[:,1].fillna(\"\") + \" \" + data.loc[:,2].fillna(\"\") + \" \" + data.loc[:,3].fillna(\"\") # 3-columns text\n",
    "    else:\n",
    "        lines = data.loc[:,1].fillna(\"\") + \" \" + data.loc[:,2].fillna(\"\") # 2-columns text\n",
    "    #print(lines)\n",
    "    \n",
    "    # I-2. Initialize the CountVectorizer with the vocabulary obtained from the training data\n",
    "    if dataset == \"sogou_news\":\n",
    "        # For Chinesse news, don't use a any stop words\n",
    "        vectorizer_test = CountVectorizer(stop_words=None, vocabulary = vectorizer_train.vocabulary_)\n",
    "    else:\n",
    "        # For the other datasets, use the English most common stop words\n",
    "        vectorizer_test = CountVectorizer(stop_words='english', vocabulary = vectorizer_train.vocabulary_)\n",
    "    \n",
    "    # I-3. Apply this CountVectorizer to the test data\n",
    "    x_test = vectorizer_test.fit_transform(lines)\n",
    "    #print(vectorizer_train.vocabulary_ == vectorizer_test.vocabulary_)\n",
    "    #print(vectorizer_train.vocabulary_)\n",
    "\n",
    "    # II. Get the target vector:\n",
    "    y_test = data.loc[:,0].as_matrix()\n",
    "    \n",
    "    # 4. Return the frequency bag-of-words for the train and test sets\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AG News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "(120000, 5000)\n",
      "(120000,)\n",
      "\n",
      "\n",
      "TEST\n",
      "(7600, 5000)\n",
      "(7600,)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = get_freqBoW_and_targetvect(\"ag_news\")\n",
    "\n",
    "print(\"TRAIN\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"\\n\\nTEST\")\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "(560000, 5000)\n",
      "(560000,)\n",
      "\n",
      "\n",
      "TEST\n",
      "(70000, 5000)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = get_freqBoW_and_targetvect(\"dbpedia\")\n",
    "\n",
    "print(\"TRAIN\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"\\n\\nTEST\")\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sogou News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "(450000, 5000)\n",
      "(450000,)\n",
      "\n",
      "\n",
      "TEST\n",
      "(60000, 5000)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = get_freqBoW_and_targetvect(\"sogou_news\")\n",
    "\n",
    "print(\"TRAIN\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"\\n\\nTEST\")\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yahoo Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "(1400000, 5000)\n",
      "(1400000,)\n",
      "\n",
      "\n",
      "TEST\n",
      "(60000, 5000)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = get_freqBoW_and_targetvect(\"yahoo\")\n",
    "\n",
    "print(\"TRAIN\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"\\n\\nTEST\")\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Reviews (Full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "(3000000, 5000)\n",
      "(3000000,)\n",
      "\n",
      "\n",
      "TEST\n",
      "(650000, 5000)\n",
      "(650000,)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = get_freqBoW_and_targetvect(\"amazon_full\")\n",
    "\n",
    "print(\"TRAIN\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"\\n\\nTEST\")\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Reviews (Polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "(3600000, 5000)\n",
      "(3600000,)\n",
      "\n",
      "\n",
      "TEST\n",
      "(400000, 5000)\n",
      "(400000,)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = get_freqBoW_and_targetvect(\"amazon_polarity\")\n",
    "\n",
    "print(\"TRAIN\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"\\n\\nTEST\")\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
